{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6efcd81",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "Use Pandas to import the data and prepare it for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7299d824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url                0\n",
      "length_url         0\n",
      "length_hostname    0\n",
      "ip                 0\n",
      "nb_dots            0\n",
      "                  ..\n",
      "web_traffic        0\n",
      "dns_record         0\n",
      "google_index       0\n",
      "page_rank          0\n",
      "status             0\n",
      "Length: 89, dtype: int64\n",
      "                                                      url    length_url  \\\n",
      "count                                               11430  11430.000000   \n",
      "unique                                              11429           NaN   \n",
      "top     http://e710z0ear.du.r.appspot.com/c:/users/use...           NaN   \n",
      "freq                                                    2           NaN   \n",
      "mean                                                  NaN     61.126684   \n",
      "std                                                   NaN     55.297318   \n",
      "min                                                   NaN     12.000000   \n",
      "25%                                                   NaN     33.000000   \n",
      "50%                                                   NaN     47.000000   \n",
      "75%                                                   NaN     71.000000   \n",
      "max                                                   NaN   1641.000000   \n",
      "\n",
      "        length_hostname            ip       nb_dots    nb_hyphens  \\\n",
      "count      11430.000000  11430.000000  11430.000000  11430.000000   \n",
      "unique              NaN           NaN           NaN           NaN   \n",
      "top                 NaN           NaN           NaN           NaN   \n",
      "freq                NaN           NaN           NaN           NaN   \n",
      "mean          21.090289      0.150569      2.480752      0.997550   \n",
      "std           10.777171      0.357644      1.369686      2.087087   \n",
      "min            4.000000      0.000000      1.000000      0.000000   \n",
      "25%           15.000000      0.000000      2.000000      0.000000   \n",
      "50%           19.000000      0.000000      2.000000      0.000000   \n",
      "75%           24.000000      0.000000      3.000000      1.000000   \n",
      "max          214.000000      1.000000     24.000000     43.000000   \n",
      "\n",
      "               nb_at         nb_qm        nb_and    nb_or         nb_eq  \\\n",
      "count   11430.000000  11430.000000  11430.000000  11430.0  11430.000000   \n",
      "unique           NaN           NaN           NaN      NaN           NaN   \n",
      "top              NaN           NaN           NaN      NaN           NaN   \n",
      "freq             NaN           NaN           NaN      NaN           NaN   \n",
      "mean        0.022222      0.141207      0.162292      0.0      0.293176   \n",
      "std         0.155500      0.364456      0.821337      0.0      0.998317   \n",
      "min         0.000000      0.000000      0.000000      0.0      0.000000   \n",
      "25%         0.000000      0.000000      0.000000      0.0      0.000000   \n",
      "50%         0.000000      0.000000      0.000000      0.0      0.000000   \n",
      "75%         0.000000      0.000000      0.000000      0.0      0.000000   \n",
      "max         4.000000      3.000000     19.000000      0.0     19.000000   \n",
      "\n",
      "        nb_underscore      nb_tilde    nb_percent      nb_slash       nb_star  \\\n",
      "count    11430.000000  11430.000000  11430.000000  11430.000000  11430.000000   \n",
      "unique            NaN           NaN           NaN           NaN           NaN   \n",
      "top               NaN           NaN           NaN           NaN           NaN   \n",
      "freq              NaN           NaN           NaN           NaN           NaN   \n",
      "mean         0.322660      0.006649      0.123097      4.289589      0.000700   \n",
      "std          1.093336      0.081274      1.466450      1.882251      0.026448   \n",
      "min          0.000000      0.000000      0.000000      2.000000      0.000000   \n",
      "25%          0.000000      0.000000      0.000000      3.000000      0.000000   \n",
      "50%          0.000000      0.000000      0.000000      4.000000      0.000000   \n",
      "75%          0.000000      0.000000      0.000000      5.000000      0.000000   \n",
      "max         18.000000      1.000000     96.000000     33.000000      1.000000   \n",
      "\n",
      "            nb_colon      nb_comma  nb_semicolumn     nb_dollar  ...  \\\n",
      "count   11430.000000  11430.000000   11430.000000  11430.000000  ...   \n",
      "unique           NaN           NaN            NaN           NaN  ...   \n",
      "top              NaN           NaN            NaN           NaN  ...   \n",
      "freq             NaN           NaN            NaN           NaN  ...   \n",
      "mean        1.027909      0.004024       0.062292      0.001925  ...   \n",
      "std         0.240325      0.103240       0.598190      0.077111  ...   \n",
      "min         1.000000      0.000000       0.000000      0.000000  ...   \n",
      "25%         1.000000      0.000000       0.000000      0.000000  ...   \n",
      "50%         1.000000      0.000000       0.000000      0.000000  ...   \n",
      "75%         1.000000      0.000000       0.000000      0.000000  ...   \n",
      "max         7.000000      4.000000      20.000000      6.000000  ...   \n",
      "\n",
      "        submit_email  ratio_intMedia  ratio_extMedia      sfh        iframe  \\\n",
      "count        11430.0    11430.000000    11430.000000  11430.0  11430.000000   \n",
      "unique           NaN             NaN             NaN      NaN           NaN   \n",
      "top              NaN             NaN             NaN      NaN           NaN   \n",
      "freq             NaN             NaN             NaN      NaN           NaN   \n",
      "mean             0.0       42.870444       23.236293      0.0      0.001312   \n",
      "std              0.0       46.249897       38.386577      0.0      0.036204   \n",
      "min              0.0        0.000000        0.000000      0.0      0.000000   \n",
      "25%              0.0        0.000000        0.000000      0.0      0.000000   \n",
      "50%              0.0       11.111111        0.000000      0.0      0.000000   \n",
      "75%              0.0      100.000000       33.333333      0.0      0.000000   \n",
      "max              0.0      100.000000      100.000000      0.0      1.000000   \n",
      "\n",
      "        popup_window   safe_anchor   onmouseover   right_clic   empty_title  \\\n",
      "count   11430.000000  11430.000000  11430.000000  11430.00000  11430.000000   \n",
      "unique           NaN           NaN           NaN          NaN           NaN   \n",
      "top              NaN           NaN           NaN          NaN           NaN   \n",
      "freq             NaN           NaN           NaN          NaN           NaN   \n",
      "mean        0.006037     37.063922      0.001137      0.00140      0.124759   \n",
      "std         0.077465     39.073385      0.033707      0.03739      0.330460   \n",
      "min         0.000000      0.000000      0.000000      0.00000      0.000000   \n",
      "25%         0.000000      0.000000      0.000000      0.00000      0.000000   \n",
      "50%         0.000000     23.294574      0.000000      0.00000      0.000000   \n",
      "75%         0.000000     75.000000      0.000000      0.00000      0.000000   \n",
      "max         1.000000    100.000000      1.000000      1.00000      1.000000   \n",
      "\n",
      "        domain_in_title  domain_with_copyright  whois_registered_domain  \\\n",
      "count      11430.000000           11430.000000             11430.000000   \n",
      "unique              NaN                    NaN                      NaN   \n",
      "top                 NaN                    NaN                      NaN   \n",
      "freq                NaN                    NaN                      NaN   \n",
      "mean           0.775853               0.439545                 0.072878   \n",
      "std            0.417038               0.496353                 0.259948   \n",
      "min            0.000000               0.000000                 0.000000   \n",
      "25%            1.000000               0.000000                 0.000000   \n",
      "50%            1.000000               0.000000                 0.000000   \n",
      "75%            1.000000               1.000000                 0.000000   \n",
      "max            1.000000               1.000000                 1.000000   \n",
      "\n",
      "        domain_registration_length    domain_age   web_traffic    dns_record  \\\n",
      "count                 11430.000000  11430.000000  1.143000e+04  11430.000000   \n",
      "unique                         NaN           NaN           NaN           NaN   \n",
      "top                            NaN           NaN           NaN           NaN   \n",
      "freq                           NaN           NaN           NaN           NaN   \n",
      "mean                    492.532196   4062.543745  8.567566e+05      0.020122   \n",
      "std                     814.769415   3107.784600  1.995606e+06      0.140425   \n",
      "min                      -1.000000    -12.000000  0.000000e+00      0.000000   \n",
      "25%                      84.000000    972.250000  0.000000e+00      0.000000   \n",
      "50%                     242.000000   3993.000000  1.651000e+03      0.000000   \n",
      "75%                     449.000000   7026.750000  3.738455e+05      0.000000   \n",
      "max                   29829.000000  12874.000000  1.076799e+07      1.000000   \n",
      "\n",
      "        google_index     page_rank      status  \n",
      "count   11430.000000  11430.000000       11430  \n",
      "unique           NaN           NaN           2  \n",
      "top              NaN           NaN  legitimate  \n",
      "freq             NaN           NaN        5715  \n",
      "mean        0.533946      3.185739         NaN  \n",
      "std         0.498868      2.536955         NaN  \n",
      "min         0.000000      0.000000         NaN  \n",
      "25%         0.000000      1.000000         NaN  \n",
      "50%         1.000000      3.000000         NaN  \n",
      "75%         1.000000      5.000000         NaN  \n",
      "max         1.000000     10.000000         NaN  \n",
      "\n",
      "[11 rows x 89 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows are 11430 and number of columns are  89\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11430 entries, 0 to 11429\n",
      "Data columns (total 89 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   url                         11430 non-null  object \n",
      " 1   length_url                  11430 non-null  int64  \n",
      " 2   length_hostname             11430 non-null  int64  \n",
      " 3   ip                          11430 non-null  int64  \n",
      " 4   nb_dots                     11430 non-null  int64  \n",
      " 5   nb_hyphens                  11430 non-null  int64  \n",
      " 6   nb_at                       11430 non-null  int64  \n",
      " 7   nb_qm                       11430 non-null  int64  \n",
      " 8   nb_and                      11430 non-null  int64  \n",
      " 9   nb_or                       11430 non-null  int64  \n",
      " 10  nb_eq                       11430 non-null  int64  \n",
      " 11  nb_underscore               11430 non-null  int64  \n",
      " 12  nb_tilde                    11430 non-null  int64  \n",
      " 13  nb_percent                  11430 non-null  int64  \n",
      " 14  nb_slash                    11430 non-null  int64  \n",
      " 15  nb_star                     11430 non-null  int64  \n",
      " 16  nb_colon                    11430 non-null  int64  \n",
      " 17  nb_comma                    11430 non-null  int64  \n",
      " 18  nb_semicolumn               11430 non-null  int64  \n",
      " 19  nb_dollar                   11430 non-null  int64  \n",
      " 20  nb_space                    11430 non-null  int64  \n",
      " 21  nb_www                      11430 non-null  int64  \n",
      " 22  nb_com                      11430 non-null  int64  \n",
      " 23  nb_dslash                   11430 non-null  int64  \n",
      " 24  http_in_path                11430 non-null  int64  \n",
      " 25  https_token                 11430 non-null  int64  \n",
      " 26  ratio_digits_url            11430 non-null  float64\n",
      " 27  ratio_digits_host           11430 non-null  float64\n",
      " 28  punycode                    11430 non-null  int64  \n",
      " 29  port                        11430 non-null  int64  \n",
      " 30  tld_in_path                 11430 non-null  int64  \n",
      " 31  tld_in_subdomain            11430 non-null  int64  \n",
      " 32  abnormal_subdomain          11430 non-null  int64  \n",
      " 33  nb_subdomains               11430 non-null  int64  \n",
      " 34  prefix_suffix               11430 non-null  int64  \n",
      " 35  random_domain               11430 non-null  int64  \n",
      " 36  shortening_service          11430 non-null  int64  \n",
      " 37  path_extension              11430 non-null  int64  \n",
      " 38  nb_redirection              11430 non-null  int64  \n",
      " 39  nb_external_redirection     11430 non-null  int64  \n",
      " 40  length_words_raw            11430 non-null  int64  \n",
      " 41  char_repeat                 11430 non-null  int64  \n",
      " 42  shortest_words_raw          11430 non-null  int64  \n",
      " 43  shortest_word_host          11430 non-null  int64  \n",
      " 44  shortest_word_path          11430 non-null  int64  \n",
      " 45  longest_words_raw           11430 non-null  int64  \n",
      " 46  longest_word_host           11430 non-null  int64  \n",
      " 47  longest_word_path           11430 non-null  int64  \n",
      " 48  avg_words_raw               11430 non-null  float64\n",
      " 49  avg_word_host               11430 non-null  float64\n",
      " 50  avg_word_path               11430 non-null  float64\n",
      " 51  phish_hints                 11430 non-null  int64  \n",
      " 52  domain_in_brand             11430 non-null  int64  \n",
      " 53  brand_in_subdomain          11430 non-null  int64  \n",
      " 54  brand_in_path               11430 non-null  int64  \n",
      " 55  suspecious_tld              11430 non-null  int64  \n",
      " 56  statistical_report          11430 non-null  int64  \n",
      " 57  nb_hyperlinks               11430 non-null  int64  \n",
      " 58  ratio_intHyperlinks         11430 non-null  float64\n",
      " 59  ratio_extHyperlinks         11430 non-null  float64\n",
      " 60  ratio_nullHyperlinks        11430 non-null  int64  \n",
      " 61  nb_extCSS                   11430 non-null  int64  \n",
      " 62  ratio_intRedirection        11430 non-null  int64  \n",
      " 63  ratio_extRedirection        11430 non-null  float64\n",
      " 64  ratio_intErrors             11430 non-null  int64  \n",
      " 65  ratio_extErrors             11430 non-null  float64\n",
      " 66  login_form                  11430 non-null  int64  \n",
      " 67  external_favicon            11430 non-null  int64  \n",
      " 68  links_in_tags               11430 non-null  float64\n",
      " 69  submit_email                11430 non-null  int64  \n",
      " 70  ratio_intMedia              11430 non-null  float64\n",
      " 71  ratio_extMedia              11430 non-null  float64\n",
      " 72  sfh                         11430 non-null  int64  \n",
      " 73  iframe                      11430 non-null  int64  \n",
      " 74  popup_window                11430 non-null  int64  \n",
      " 75  safe_anchor                 11430 non-null  float64\n",
      " 76  onmouseover                 11430 non-null  int64  \n",
      " 77  right_clic                  11430 non-null  int64  \n",
      " 78  empty_title                 11430 non-null  int64  \n",
      " 79  domain_in_title             11430 non-null  int64  \n",
      " 80  domain_with_copyright       11430 non-null  int64  \n",
      " 81  whois_registered_domain     11430 non-null  int64  \n",
      " 82  domain_registration_length  11430 non-null  int64  \n",
      " 83  domain_age                  11430 non-null  int64  \n",
      " 84  web_traffic                 11430 non-null  int64  \n",
      " 85  dns_record                  11430 non-null  int64  \n",
      " 86  google_index                11430 non-null  int64  \n",
      " 87  page_rank                   11430 non-null  int64  \n",
      " 88  status                      11430 non-null  object \n",
      "dtypes: float64(13), int64(74), object(2)\n",
      "memory usage: 7.8+ MB\n",
      "None\n",
      "[0 1 1 ... 0 0 1]\n",
      "[[ 37.  19.   0. ...   1.   4.   1.]\n",
      " [ 77.  23.   1. ...   1.   2.   0.]\n",
      " [126.  50.   1. ...   1.   0.   0.]\n",
      " ...\n",
      " [105.  16.   1. ...   1.  10.   1.]\n",
      " [ 38.  30.   0. ...   0.   4.   1.]\n",
      " [477.  14.   1. ...   1.   0.   0.]]\n",
      "(11430, 88) (11430,)\n",
      "X_train shape = (6858, 88), X_val shape = (2286, 88), X_test shape = (2286, 88)\n",
      "y_train shape = (6858,), y_val shape = (2286,), y_test shape = (2286,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('Dataset/dataset_phishing.csv')\n",
    "\n",
    "#print null\n",
    "print(df.isnull().sum())\n",
    "\n",
    "#remove all null\n",
    "df = df.dropna()\n",
    "\n",
    "#describe statistical data. To stdout\n",
    "with pd.option_context('display.max_columns', 40):\n",
    "    print(df.describe(include='all'))\n",
    "\n",
    "#describe statistical data to text file: out.txt\n",
    "with open('out.txt', 'w') as f:\n",
    "    with pd.option_context('display.max_columns', 40):\n",
    "        print(df.describe(include='all'),file=f)\n",
    "        \n",
    "#shape of data\n",
    "print('Number of rows are',df.shape[0], 'and number of columns are ',df.shape[1])\n",
    "\n",
    "#look at data types of columns\n",
    "print(df.info())\n",
    "\n",
    "#Pair plot (takes way too long, we need to decide on important features first)\n",
    "# plt.figure(figsize=(20,20))\n",
    "# sns.pairplot(df)\n",
    "# plt.savefig('./Figures/pairplot.png')\n",
    "# plt.show()\n",
    "\n",
    "#encode last column \n",
    "dummy_data = pd.get_dummies(df, columns = ['status'])\n",
    "\n",
    "\n",
    "#Get X and y \n",
    "X = dummy_data.iloc[:,1:-1].values\n",
    "y = dummy_data.iloc[:,-1].values\n",
    "\n",
    "#try printing last column to make sure it's binary\n",
    "print(y)\n",
    "print(X)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "#Train, test, val split. 60%, 20%, 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "#Doing test_train split for now, later cross validation\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "print(f\"X_train shape = {X_train.shape}, X_val shape = {X_val.shape}, X_test shape = {X_test.shape}\")\n",
    "print(f\"y_train shape = {y_train.shape}, y_val shape = {y_val.shape}, y_test shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704642fd",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "065ff3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9986876640419947\n",
      "Validation Score: n_estimators = 10, max_leaf = 10 , random = 0, Score = 1.0\n",
      "Training Score: 0.9720034995625547\n",
      "Validation Score: n_estimators = 10, max_leaf = 10 , random = 20, Score = 0.9015748031496063\n",
      "Training Score: 0.9927092446777486\n",
      "Validation Score: n_estimators = 10, max_leaf = 10 , random = 40, Score = 1.0\n",
      "Training Score: 0.9998541848935549\n",
      "Validation Score: n_estimators = 10, max_leaf = 50 , random = 0, Score = 1.0\n",
      "Training Score: 0.9918343540390785\n",
      "Validation Score: n_estimators = 10, max_leaf = 50 , random = 20, Score = 0.7125984251968503\n",
      "Training Score: 0.9981044036162147\n",
      "Validation Score: n_estimators = 10, max_leaf = 50 , random = 40, Score = 1.0\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 10, max_leaf = None , random = 0, Score = 1.0\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 10, max_leaf = None , random = 20, Score = 0.9969378827646544\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 10, max_leaf = None , random = 40, Score = 1.0\n",
      "Training Score: 0.9928550597841936\n",
      "Validation Score: n_estimators = 100, max_leaf = 10 , random = 0, Score = 1.0\n",
      "Training Score: 0.9982502187226596\n",
      "Validation Score: n_estimators = 100, max_leaf = 10 , random = 20, Score = 1.0\n",
      "Training Score: 0.9978127734033246\n",
      "Validation Score: n_estimators = 100, max_leaf = 10 , random = 40, Score = 1.0\n",
      "Training Score: 0.9988334791484398\n",
      "Validation Score: n_estimators = 100, max_leaf = 50 , random = 0, Score = 1.0\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 100, max_leaf = 50 , random = 20, Score = 1.0\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 100, max_leaf = 50 , random = 40, Score = 1.0\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 100, max_leaf = None , random = 0, Score = 1.0\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 100, max_leaf = None , random = 20, Score = 1.0\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 100, max_leaf = None , random = 40, Score = 1.0\n",
      "Training Score: 0.9916885389326334\n",
      "Validation Score: n_estimators = 500, max_leaf = 10 , random = 0, Score = 1.0\n",
      "Training Score: 0.9966462525517643\n",
      "Validation Score: n_estimators = 500, max_leaf = 10 , random = 20, Score = 1.0\n",
      "Training Score: 0.9940215806357539\n",
      "Validation Score: n_estimators = 500, max_leaf = 10 , random = 40, Score = 1.0\n",
      "Training Score: 0.9994167395742198\n",
      "Validation Score: n_estimators = 500, max_leaf = 50 , random = 0, Score = 1.0\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 500, max_leaf = 50 , random = 20, Score = 1.0\n",
      "Training Score: 0.99970836978711\n",
      "Validation Score: n_estimators = 500, max_leaf = 50 , random = 40, Score = 1.0\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 500, max_leaf = None , random = 0, Score = 1.0\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 500, max_leaf = None , random = 20, Score = 1.0\n",
      "Training Score: 1.0\n",
      "Validation Score: n_estimators = 500, max_leaf = None , random = 40, Score = 1.0\n",
      "Best Model: n_estimators = 10, max_leaf = 10 , random = 0, Score = 1.0\n",
      "Best Model Test Score: 0.9986876640419947\n",
      "RandomForestClassifier(n_estimators=500, random_state=40)\n",
      "RandomForestClassifier(max_leaf_nodes=50, n_estimators=10, random_state=20)\n"
     ]
    }
   ],
   "source": [
    "#Feature scaling. Do we need this??\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "#just transform because we already did a fit above ^\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "#Random Forest\n",
    "#default = 100\n",
    "estimators = [10, 100, 500]\n",
    "#default = None\n",
    "max_leaf = [10, 50, None]\n",
    "\n",
    "random_state = [0,20,40]\n",
    "bestModel = 0\n",
    "bestScore = 0\n",
    "bestParams = [0,0,0]\n",
    "\n",
    "#parellel arrays, so I can have the best, and also some worse models\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "\n",
    "#will probably want to do some learning curves here \n",
    "for e in estimators:\n",
    "    for m in max_leaf:\n",
    "        for r in random_state:\n",
    "            model = ensemble.RandomForestClassifier(n_estimators=e, max_leaf_nodes=m, random_state=r)\n",
    "            model.fit(X_train,y_train.flatten())\n",
    "            print(f\"Training Score: {model.score(X_train,y_train)}\")\n",
    "            # it makes predictions using X_test under the hood and uses those predictions to calculate accuracy score\n",
    "            score = model.score(X_val,y_val)\n",
    "            models.append(model)\n",
    "            scores.append(score)\n",
    "            print(f\"Validation Score: n_estimators = {e}, max_leaf = {m} , random = {r}, Score = {score}\")\n",
    "            if score > bestScore:\n",
    "                bestModel = model\n",
    "                bestScore = score\n",
    "                bestParams = [e,m,r]\n",
    "                \n",
    "            \n",
    "\n",
    "print(f\"Best Model: n_estimators = {bestParams[0]}, max_leaf = {bestParams[1]} , random = {bestParams[2]}, Score = {bestScore}\")\n",
    "\n",
    "#check test score of best model\n",
    "print(f\"Best Model Test Score: {bestModel.score(X_test, y_test)}\")\n",
    "\n",
    "scores = np.array(scores)\n",
    "models = np.array(models, dtype=object)\n",
    "\n",
    "#sorts in ascending order\n",
    "bestIndices = np.argsort(scores)\n",
    "ascendingModels = []\n",
    "for i in bestIndices:\n",
    "    ascendingModels.append(models[i])\n",
    "    \n",
    "print(ascendingModels[-1])\n",
    "bestModel2 = ascendingModels[-1]\n",
    "print(ascendingModels[0])\n",
    "worstModel = ascendingModels[0]\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46225be",
   "metadata": {},
   "source": [
    "## Learning Curves for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fd2dc8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Learning curve for best model (and maybe try another model or 2?)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# reccomended - a function to compute data and draw the learning curves\n",
    "def plot_learning_curves(model, Xtrain, ytrain, Xval, yval, ax):\n",
    "    \n",
    "    mTrain = Xtrain.shape[0]\n",
    "    \n",
    "    #will be used to graph later\n",
    "    Jtrain = []\n",
    "    Jval = []\n",
    "    \n",
    "    trainingExamples = 20\n",
    "    \n",
    "    #loop up to mTrain, taking small subsets of the train data set from 1-mTrain\n",
    "    for m in range(1,trainingExamples):\n",
    "        #create subset of training data\n",
    "        XTrainTemp = Xtrain[0:m,:]\n",
    "        yTrainTemp = ytrain[0:m]\n",
    "\n",
    "        model.fit(Xtrain, ytrain.flatten())\n",
    "        yTrainPred = model.predict(XTrainTemp)\n",
    "        #test validation on the whole data set\n",
    "        yValPred = model.predict(Xval)\n",
    "#         print(yTrainTemp.shape)\n",
    "#         print(yTrainPred.shape)\n",
    "        \n",
    "        #squared = False -> RMSE rather than MSE\n",
    "        Jtrain.append(mean_squared_error(yTrainTemp,yTrainPred, squared=False ))\n",
    "        #also need to test validation on the whole set\n",
    "        Jval.append(mean_squared_error(yval,yValPred, squared=False ))\n",
    "    #plot\n",
    "    ax.set_xlabel('Training Set Size')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    i = [int(x) for x in range(1,trainingExamples)]\n",
    "    ax.plot(i,Jtrain,\"r-|\",label=\"train\")\n",
    "    ax.plot(i,Jval,\"bx-\",label=\"val\")\n",
    "    ax.legend(loc = 'best')\n",
    "    \n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.set_title(\"Worst Model\")\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.set_title(\"Best Model\")\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.set_title(\"Best Model 2\")\n",
    "ax2.set_ylim(0,0.5);\n",
    "ax3.set_ylim(0,0.5);\n",
    "\n",
    "# ax3.set_ylim(0,200);\n",
    "    \n",
    "#Learning Curvues for Random Forest \n",
    "\n",
    "#worst model, a litle underfitting\n",
    "plot_learning_curves(worstModel, X_train, y_train, X_val, y_val, ax1)\n",
    "#next 2 are both 100%\n",
    "plot_learning_curves(bestModel, X_train, y_train, X_val, y_val, ax2)\n",
    "plot_learning_curves(bestModel2, X_train, y_train, X_val, y_val, ax3)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a49689",
   "metadata": {},
   "source": [
    "## Feature Importance for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3eca3b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the 10 best features\n",
      "page_rank\n",
      "dns_record\n",
      "google_index\n",
      "ratio_intRedirection\n",
      "domain_age\n",
      "ratio_intHyperlinks\n",
      "popup_window\n",
      "avg_word_path\n",
      "nb_space\n",
      "url\n"
     ]
    }
   ],
   "source": [
    "#probabilities that determine each feature's importance\n",
    "featureImportance = bestModel.feature_importances_\n",
    "# print(featureImportance)\n",
    "# print(featureImportance.shape)\n",
    "\n",
    "#in ascneding order\n",
    "sortedIndices = np.argsort(featureImportance)\n",
    "descendingIndices = np.flip(sortedIndices,0)\n",
    "\n",
    "#a list that goes from best -> worst features\n",
    "bestFeatures = []\n",
    "for i in descendingIndices:\n",
    "    bestFeatures.append(df.columns[i])\n",
    "    \n",
    "print(\"Here are the 10 best features\")\n",
    "for i in range(10):\n",
    "    print(bestFeatures[i])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "641ba0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Visualization With Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32122ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,20))\n",
    "sns.pairplot(df, vars = bestFeatures)\n",
    "plt.savefig('./Figures/pairplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a050e1f",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km1 = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "#This doesn't really make sense. Hmmm\n",
    "print(km1.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337f758",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7109a052",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for linear kernel\n",
      "Score for linear kernel with C = 0.01 and sigma = 1.0: Train data - 0.5231846019247594 Val data - 0.5258092738407699\n",
      "Score for linear kernel with C = 0.03 and sigma = 1.0: Train data - 0.5231846019247594 Val data - 0.5258092738407699\n",
      "Score for linear kernel with C = 0.1 and sigma = 1.0: Train data - 0.5384951881014873 Val data - 0.5389326334208224\n",
      "Score for linear kernel with C = 0.3 and sigma = 1.0: Train data - 0.6140274132400116 Val data - 0.568241469816273\n",
      "Score for linear kernel with C = 1 and sigma = 1.0: Train data - 1.0 Val data - 0.668416447944007\n",
      "Score for linear kernel with C = 2 and sigma = 1.0: Train data - 1.0 Val data - 0.678477690288714\n",
      "Score for linear kernel with C = 0.01 and sigma = 0.04: Train data - 0.8401866433362496 Val data - 0.8363954505686789\n",
      "Score for linear kernel with C = 0.03 and sigma = 0.04: Train data - 0.9180519101778944 Val data - 0.9098862642169728\n",
      "Score for linear kernel with C = 0.1 and sigma = 0.04: Train data - 0.9549431321084865 Val data - 0.9553805774278216\n",
      "Score for linear kernel with C = 0.3 and sigma = 0.04: Train data - 0.984251968503937 Val data - 0.973753280839895\n",
      "Score for linear kernel with C = 1 and sigma = 0.04: Train data - 0.9998541848935549 Val data - 0.989501312335958\n",
      "Score for linear kernel with C = 2 and sigma = 0.04: Train data - 1.0 Val data - 0.989501312335958\n",
      "Score for linear kernel with C = 0.01 and sigma = 0.0025: Train data - 0.9619422572178478 Val data - 0.968503937007874\n",
      "Score for linear kernel with C = 0.03 and sigma = 0.0025: Train data - 0.9848352289297171 Val data - 0.9868766404199475\n",
      "Score for linear kernel with C = 0.1 and sigma = 0.0025: Train data - 0.9956255468066492 Val data - 0.9960629921259843\n",
      "Score for linear kernel with C = 0.3 and sigma = 0.0025: Train data - 0.9972295129775445 Val data - 0.9965004374453194\n",
      "Score for linear kernel with C = 1 and sigma = 0.0025: Train data - 0.9992709244677749 Val data - 0.9982502187226596\n",
      "Score for linear kernel with C = 2 and sigma = 0.0025: Train data - 0.99970836978711 Val data - 0.9986876640419947\n",
      "Score for linear kernel with C = 0.01 and sigma = 0.0004: Train data - 0.668708078156897 Val data - 0.6767279090113736\n",
      "Score for linear kernel with C = 0.03 and sigma = 0.0004: Train data - 0.9295713035870516 Val data - 0.9422572178477691\n",
      "Score for linear kernel with C = 0.1 and sigma = 0.0004: Train data - 0.9826480023330417 Val data - 0.984251968503937\n",
      "Score for linear kernel with C = 0.3 and sigma = 0.0004: Train data - 0.9988334791484398 Val data - 0.9986876640419947\n",
      "Score for linear kernel with C = 1 and sigma = 0.0004: Train data - 1.0 Val data - 0.9995625546806649\n",
      "Score for linear kernel with C = 2 and sigma = 0.0004: Train data - 1.0 Val data - 0.9995625546806649\n",
      "Score for linear kernel with C = 0.01 and sigma = 0.0001: Train data - 0.5298920968212307 Val data - 0.531058617672791\n",
      "Score for linear kernel with C = 0.03 and sigma = 0.0001: Train data - 0.5298920968212307 Val data - 0.531058617672791\n",
      "Score for linear kernel with C = 0.1 and sigma = 0.0001: Train data - 0.894575678040245 Val data - 0.9063867016622922\n",
      "Score for linear kernel with C = 0.3 and sigma = 0.0001: Train data - 0.9775444736074658 Val data - 0.9798775153105862\n",
      "Score for linear kernel with C = 1 and sigma = 0.0001: Train data - 0.9975211431904345 Val data - 0.9982502187226596\n",
      "Score for linear kernel with C = 2 and sigma = 0.0001: Train data - 1.0 Val data - 0.9991251093613298\n",
      "Score for linear kernel with C = 0.01 and sigma = 2.5e-05: Train data - 0.5150189559638378 Val data - 0.5170603674540682\n",
      "Score for linear kernel with C = 0.03 and sigma = 2.5e-05: Train data - 0.5150189559638378 Val data - 0.5170603674540682\n",
      "Score for linear kernel with C = 0.1 and sigma = 2.5e-05: Train data - 0.5150189559638378 Val data - 0.5170603674540682\n",
      "Score for linear kernel with C = 0.3 and sigma = 2.5e-05: Train data - 0.8126275882181394 Val data - 0.8184601924759405\n",
      "Score for linear kernel with C = 1 and sigma = 2.5e-05: Train data - 0.9731700204141149 Val data - 0.9781277340332458\n",
      "Score for linear kernel with C = 2 and sigma = 2.5e-05: Train data - 0.9883347914843977 Val data - 0.989501312335958\n",
      "\n",
      "Values for rbf kernel\n",
      "Score for rbf kernel with C = 0.01 and sigma = 1.0: Train data - 0.5231846019247594 Val data - 0.5258092738407699\n",
      "Score for rbf kernel with C = 0.03 and sigma = 1.0: Train data - 0.5231846019247594 Val data - 0.5258092738407699\n",
      "Score for rbf kernel with C = 0.1 and sigma = 1.0: Train data - 0.5384951881014873 Val data - 0.5389326334208224\n",
      "Score for rbf kernel with C = 0.3 and sigma = 1.0: Train data - 0.6140274132400116 Val data - 0.568241469816273\n",
      "Score for rbf kernel with C = 1 and sigma = 1.0: Train data - 1.0 Val data - 0.668416447944007\n",
      "Score for rbf kernel with C = 2 and sigma = 1.0: Train data - 1.0 Val data - 0.678477690288714\n",
      "Score for rbf kernel with C = 0.01 and sigma = 0.04: Train data - 0.8401866433362496 Val data - 0.8363954505686789\n",
      "Score for rbf kernel with C = 0.03 and sigma = 0.04: Train data - 0.9180519101778944 Val data - 0.9098862642169728\n",
      "Score for rbf kernel with C = 0.1 and sigma = 0.04: Train data - 0.9549431321084865 Val data - 0.9553805774278216\n",
      "Score for rbf kernel with C = 0.3 and sigma = 0.04: Train data - 0.984251968503937 Val data - 0.973753280839895\n",
      "Score for rbf kernel with C = 1 and sigma = 0.04: Train data - 0.9998541848935549 Val data - 0.989501312335958\n",
      "Score for rbf kernel with C = 2 and sigma = 0.04: Train data - 1.0 Val data - 0.989501312335958\n",
      "Score for rbf kernel with C = 0.01 and sigma = 0.0025: Train data - 0.9619422572178478 Val data - 0.968503937007874\n",
      "Score for rbf kernel with C = 0.03 and sigma = 0.0025: Train data - 0.9848352289297171 Val data - 0.9868766404199475\n",
      "Score for rbf kernel with C = 0.1 and sigma = 0.0025: Train data - 0.9956255468066492 Val data - 0.9960629921259843\n",
      "Score for rbf kernel with C = 0.3 and sigma = 0.0025: Train data - 0.9972295129775445 Val data - 0.9965004374453194\n",
      "Score for rbf kernel with C = 1 and sigma = 0.0025: Train data - 0.9992709244677749 Val data - 0.9982502187226596\n",
      "Score for rbf kernel with C = 2 and sigma = 0.0025: Train data - 0.99970836978711 Val data - 0.9986876640419947\n",
      "Score for rbf kernel with C = 0.01 and sigma = 0.0004: Train data - 0.668708078156897 Val data - 0.6767279090113736\n",
      "Score for rbf kernel with C = 0.03 and sigma = 0.0004: Train data - 0.9295713035870516 Val data - 0.9422572178477691\n",
      "Score for rbf kernel with C = 0.1 and sigma = 0.0004: Train data - 0.9826480023330417 Val data - 0.984251968503937\n",
      "Score for rbf kernel with C = 0.3 and sigma = 0.0004: Train data - 0.9988334791484398 Val data - 0.9986876640419947\n",
      "Score for rbf kernel with C = 1 and sigma = 0.0004: Train data - 1.0 Val data - 0.9995625546806649\n",
      "Score for rbf kernel with C = 2 and sigma = 0.0004: Train data - 1.0 Val data - 0.9995625546806649\n",
      "Score for rbf kernel with C = 0.01 and sigma = 0.0001: Train data - 0.5298920968212307 Val data - 0.531058617672791\n",
      "Score for rbf kernel with C = 0.03 and sigma = 0.0001: Train data - 0.5298920968212307 Val data - 0.531058617672791\n",
      "Score for rbf kernel with C = 0.1 and sigma = 0.0001: Train data - 0.894575678040245 Val data - 0.9063867016622922\n",
      "Score for rbf kernel with C = 0.3 and sigma = 0.0001: Train data - 0.9775444736074658 Val data - 0.9798775153105862\n",
      "Score for rbf kernel with C = 1 and sigma = 0.0001: Train data - 0.9975211431904345 Val data - 0.9982502187226596\n",
      "Score for rbf kernel with C = 2 and sigma = 0.0001: Train data - 1.0 Val data - 0.9991251093613298\n",
      "Score for rbf kernel with C = 0.01 and sigma = 2.5e-05: Train data - 0.5150189559638378 Val data - 0.5170603674540682\n",
      "Score for rbf kernel with C = 0.03 and sigma = 2.5e-05: Train data - 0.5150189559638378 Val data - 0.5170603674540682\n",
      "Score for rbf kernel with C = 0.1 and sigma = 2.5e-05: Train data - 0.5150189559638378 Val data - 0.5170603674540682\n",
      "Score for rbf kernel with C = 0.3 and sigma = 2.5e-05: Train data - 0.8126275882181394 Val data - 0.8184601924759405\n",
      "Score for rbf kernel with C = 1 and sigma = 2.5e-05: Train data - 0.9731700204141149 Val data - 0.9781277340332458\n",
      "Score for rbf kernel with C = 2 and sigma = 2.5e-05: Train data - 0.9883347914843977 Val data - 0.989501312335958\n",
      "\n",
      "Best params: kernel = rbf, C = 2, gamma = 200\n"
     ]
    }
   ],
   "source": [
    "svmS = StandardScaler()\n",
    "X_trains = svmS.fit_transform(X_train)\n",
    "X_vals = svmS.transform(X_val)\n",
    "\n",
    "#TODO is not outputting best params, just last params\n",
    "\n",
    "#literally no change in val when C > 2, 2 is the max\n",
    "cVals = [0.01, 0.03, 0.1, 0.3, 1, 2]\n",
    "#Try with higher gamma\n",
    "gammaVals = [1, 5, 20, 50, 100, 200]\n",
    "score = 0\n",
    "bestParams = []\n",
    "#Linear kernel\n",
    "print(\"Values for linear kernel\")\n",
    "for i in gammaVals:\n",
    "    gamma = 1/(i**2)\n",
    "    for j in cVals:\n",
    "        model = svm.SVC(C=j, gamma=gamma)\n",
    "        model.fit(X_trains, y_train.flatten())\n",
    "        if model.score(X_vals, y_val.flatten()) > score:\n",
    "            bestParams = [\"linear\", j, i]\n",
    "        print(f\"Score for linear kernel with C = {j} and sigma = {gamma}: Train data - {model.score(X_trains, y_train.flatten())} Val data - {model.score(X_vals, y_val.flatten())}\")\n",
    "        \n",
    "#rbf kernel\n",
    "print(\"\\nValues for rbf kernel\")\n",
    "for i in gammaVals:\n",
    "    gamma = 1/(i**2)\n",
    "    for j in cVals:\n",
    "        model = svm.SVC(kernel='rbf', C=j, gamma=gamma)\n",
    "        model.fit(X_trains, y_train.flatten())\n",
    "        if model.score(X_vals, y_val.flatten()) > score:\n",
    "            bestParams = [\"rbf\", j, i]\n",
    "        print(f\"Score for rbf kernel with C = {j} and sigma = {gamma}: Train data - {model.score(X_trains, y_train.flatten())} Val data - {model.score(X_vals, y_val.flatten())}\")\n",
    "        \n",
    "print(f\"\\nBest params: kernel = {bestParams[0]}, C = {bestParams[1]}, gamma = {bestParams[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4dbb5d",
   "metadata": {},
   "source": [
    "## Cross Validation on Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in estimators[:-2]:\n",
    "    for m in max_leaf[:-2]:\n",
    "        for r in random_state[:-2]:\n",
    "            model = ensemble.RandomForestClassifier(n_estimators=e, max_leaf_nodes=m, random_state=r)\n",
    "            model.fit(X_train,y_train.flatten())\n",
    "            # it makes predictions using X_test under the hood and uses those predictions to calculate accuracy score\n",
    "            score = cross_val_score(model, X, y.flatten(), cv=5)\n",
    "            print(f\"model - estimators = {e}; max_leaf = {m}; random_state = {r}:{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac878f",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "375d18c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l2 penalty:\n",
      "Logistic regression using l2 and C=0.01: Train - 0.99970836978711; Val - 0.9991251093613298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l2 and C=0.03: Train - 1.0; Val - 0.9995625546806649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l2 and C=0.1: Train - 1.0; Val - 0.9995625546806649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l2 and C=0.3: Train - 1.0; Val - 0.9995625546806649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l2 and C=1: Train - 1.0; Val - 0.9995625546806649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l2 and C=2: Train - 1.0; Val - 0.9995625546806649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l2 and C=5: Train - 1.0; Val - 0.9995625546806649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l2 and C=10: Train - 1.0; Val - 0.9995625546806649\n",
      "\n",
      "Logistic regression using l1 penalty:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l1 and C=0.01: Train - 1.0; Val - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l1 and C=0.03: Train - 1.0; Val - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l1 and C=0.1: Train - 1.0; Val - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l1 and C=0.3: Train - 1.0; Val - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l1 and C=1: Train - 1.0; Val - 0.9995625546806649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l1 and C=2: Train - 1.0; Val - 0.9995625546806649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l1 and C=5: Train - 1.0; Val - 0.9995625546806649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using l1 and C=10: Train - 1.0; Val - 0.9995625546806649\n",
      "\n",
      "Logistic regression using elastic net:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using elastic net and C=0.01: Train - 1.0; Val - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using elastic net and C=0.03: Train - 1.0; Val - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using elastic net and C=0.1: Train - 1.0; Val - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using elastic net and C=0.3: Train - 1.0; Val - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using elastic net and C=1: Train - 1.0; Val - 0.9995625546806649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using elastic net and C=2: Train - 1.0; Val - 0.9995625546806649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using elastic net and C=5: Train - 1.0; Val - 0.9995625546806649\n",
      "Logistic regression using elastic net and C=10: Train - 1.0; Val - 0.9995625546806649\n",
      "\n",
      "The best performing model uses the l1 penalty with C=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logRS = StandardScaler()\n",
    "X_trains = logRS.fit_transform(X_train)\n",
    "X_vals = logRS.transform(X_val)\n",
    "\n",
    "cVals = [0.01, 0.03, 0.1, 0.3, 1, 2, 5, 10]\n",
    "\n",
    "score = -1\n",
    "bestParams = []\n",
    "#Logistic regression with l2\n",
    "print(\"Logistic regression using l2 penalty:\")\n",
    "for i in cVals:\n",
    "    model = linear_model.LogisticRegression(C=i, solver='saga')\n",
    "    model.fit(X_trains, y_train)\n",
    "    print(f\"Logistic regression using l2 and C={i}: Train - {model.score(X_trains, y_train)}; Val - {model.score(X_vals, y_val)}\")\n",
    "    if model.score(X_vals, y_val) > score or score == -1:\n",
    "        score = model.score(X_vals, y_val)\n",
    "        bestParams = ['l2', i]\n",
    "    \n",
    "\n",
    "#Logistic regression with l1\n",
    "print(\"\\nLogistic regression using l1 penalty:\")\n",
    "for i in cVals:\n",
    "    model = linear_model.LogisticRegression(C=i, penalty='l1', solver='saga')\n",
    "    model.fit(X_trains, y_train)\n",
    "    print(f\"Logistic regression using l1 and C={i}: Train - {model.score(X_trains, y_train)}; Val - {model.score(X_vals, y_val)}\")\n",
    "    if model.score(X_vals, y_val) > score or score == -1:\n",
    "        score = model.score(X_vals, y_val)\n",
    "        bestParams = ['l1', i]\n",
    "        \n",
    "#Logistic regression with elastic net:\n",
    "print(\"\\nLogistic regression using elastic net:\")\n",
    "for i in cVals:\n",
    "    model = linear_model.LogisticRegression(C=i, penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
    "    model.fit(X_trains, y_train)\n",
    "    print(f\"Logistic regression using elastic net and C={i}: Train - {model.score(X_trains, y_train)}; Val - {model.score(X_vals, y_val)}\")\n",
    "    if model.score(X_vals, y_val) > score or score == -1:\n",
    "        score = model.score(X_vals, y_val)\n",
    "        bestParams = ['elasticnet', i]\n",
    "        \n",
    "print(f\"\\nThe best performing model uses the {bestParams[0]} penalty with C={bestParams[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de686618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
