{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be315966",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "Use Pandas to import the data and prepare it for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "df = pd.read_csv('Dataset/dataset_phishing.csv')\n",
    "\n",
    "#print null\n",
    "print(df.isnull().sum())\n",
    "\n",
    "#remove all null\n",
    "df = df.dropna()\n",
    "\n",
    "#Trying absolute value on domain_age\n",
    "df.domain_age = df.domain_age.abs()\n",
    "print(f\"Minimum domain age: {np.min(df.domain_age)}\")\n",
    "\n",
    "#describe statistical data. To stdout\n",
    "with pd.option_context('display.max_columns', 40):\n",
    "    print(df.describe(include='all'))\n",
    "\n",
    "#describe statistical data to text file: out.txt\n",
    "with open('out.txt', 'w') as f:\n",
    "    with pd.option_context('display.max_columns', 40):\n",
    "        print(df.describe(include='all'),file=f)\n",
    "        \n",
    "#shape of data\n",
    "print('Number of rows are',df.shape[0], 'and number of columns are ',df.shape[1])\n",
    "\n",
    "#look at data types of columns\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "\n",
    "#encode last column \n",
    "dummy_data = pd.get_dummies(df, columns = ['status'])\n",
    "\n",
    "#describe statistical data. To stdout\n",
    "print(\"Here is the dummy encoded data\")\n",
    "with pd.option_context('display.max_columns', 40):\n",
    "    print(dummy_data.describe(include='all'))\n",
    "\n",
    "\n",
    "#Get X and y \n",
    "X = dummy_data.iloc[:,1:-1].values\n",
    "y = dummy_data.iloc[:,-1].values\n",
    "\n",
    "#try printing last column to make sure it's binary\n",
    "print(\"Here are the labels\")\n",
    "print(y)\n",
    "print(\"Here is X\")\n",
    "print(X)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "#Shows that the data set is balanced. 50% 50% \n",
    "print(f\"Here are the number of legitamate URLs {np.sum(y==0)}. Or {np.sum(y==0)/y.shape[0]}%\")\n",
    "print(f\"Here are the number of phishing URLs {np.sum(y==1)}. Or {np.sum(y==1)/y.shape[0]}%\")\n",
    "\n",
    "#Train, test, val split. 60%, 20%, 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "#Doing test_train split for now, later cross validation\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "print(f\"X_train shape = {X_train.shape}, X_val shape = {X_val.shape}, X_test shape = {X_test.shape}\")\n",
    "print(f\"y_train shape = {y_train.shape}, y_val shape = {y_val.shape}, y_test shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ede40",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to do feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_trains = scaler.fit_transform(X_train)\n",
    "#just transform because we already did a fit above ^\n",
    "X_tests = scaler.transform(X_test)\n",
    "X_vals = scaler.transform(X_val)\n",
    "\n",
    "#Random Forest\n",
    "#default = 100\n",
    "estimators = [10, 100, 500]\n",
    "#default = None\n",
    "max_leaf = [10, 50, None]\n",
    "\n",
    "random_state = [0,20,40]\n",
    "bestModel = 0\n",
    "bestScore = 0\n",
    "bestParams = [0,0,0]\n",
    "\n",
    "#parellel arrays, so I can have the best, and also some worse models\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "\n",
    "#will probably want to do some learning curves here \n",
    "for e in estimators:\n",
    "    for m in max_leaf:\n",
    "        for r in random_state:\n",
    "            model = ensemble.RandomForestClassifier(n_estimators=e, max_leaf_nodes=m, random_state=r)\n",
    "            model.fit(X_trains,y_train.flatten())\n",
    "            print(f\"Training Score: {model.score(X_trains,y_train)}\")\n",
    "            # it makes predictions using X_test under the hood and uses those predictions to calculate accuracy score\n",
    "            score = model.score(X_vals,y_val)\n",
    "            models.append(model)\n",
    "            scores.append(score)\n",
    "            print(f\"Validation Score: n_estimators = {e}, max_leaf = {m} , random = {r}, Score = {score}\")\n",
    "            if score > bestScore:\n",
    "                bestModel = model\n",
    "                bestScore = score\n",
    "                bestParams = [e,m,r]\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "scores = np.array(scores)\n",
    "models = np.array(models, dtype=object)\n",
    "\n",
    "#sorts in ascending order\n",
    "bestIndices = np.argsort(scores)\n",
    "ascendingModels = []\n",
    "for i in bestIndices:\n",
    "    ascendingModels.append(models[i])\n",
    "    \n",
    "# print(ascendingModels[-1])\n",
    "bestModel2 = ascendingModels[-1]\n",
    "# print(ascendingModels[0])\n",
    "worstModel = ascendingModels[0]\n",
    "\n",
    "print(f\"Worst Model: n_estimators = {worstModel.n_estimators}, max_leaf = {10} , random = {worstModel.random_state}, Score = {worstModel.score(X_vals,y_val)}\")\n",
    "#check test score of best model\n",
    "print(f\"Worst Model Test Score: {worstModel.score(X_tests, y_test)}\")\n",
    "\n",
    "print(f\"Best Model: n_estimators = {bestParams[0]}, max_leaf = {bestParams[1]} , random = {bestParams[2]}, Score = {bestScore}\")\n",
    "#check test score of best model\n",
    "print(f\"Best Model Test Score: {bestModel.score(X_tests, y_test)}\")\n",
    "\n",
    "print(bestModel2)\n",
    "print(f\"Best Model2: n_estimators = {bestModel2.n_estimators}, max_leaf = {None} , random = {bestModel2.random_state}, Score = {bestModel2.score(X_vals,y_val)}\")\n",
    "#check test score of best model\n",
    "print(f\"Best Model Test Score: {bestModel2.score(X_tests, y_test)}\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5c57b",
   "metadata": {},
   "source": [
    "## Learning Curves for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Learning curve for best model (and maybe try another model or 2?)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# reccomended - a function to compute data and draw the learning curves\n",
    "def plot_learning_curves(model, Xtrain, ytrain, Xval, yval, ax):\n",
    "    \n",
    "    mTrain = Xtrain.shape[0]\n",
    "    \n",
    "    #will be used to graph later\n",
    "    Jtrain = []\n",
    "    Jval = []\n",
    "    \n",
    "    trainingExamples = mTrain\n",
    "    \n",
    "    #loop up to mTrain, taking small subsets of the train data set from 1-mTrain\n",
    "    for m in range(1,trainingExamples, 100):\n",
    "        #create subset of training data\n",
    "        XTrainTemp = Xtrain[0:m,:]\n",
    "        yTrainTemp = ytrain[0:m]\n",
    "\n",
    "        model.fit(Xtrain, ytrain.flatten())\n",
    "        yTrainPred = model.predict(XTrainTemp)\n",
    "        #test validation on the whole data set\n",
    "        yValPred = model.predict(Xval)\n",
    "#         print(yTrainTemp.shape)\n",
    "#         print(yTrainPred.shape)\n",
    "        \n",
    "        #squared = False -> RMSE rather than MSE\n",
    "        Jtrain.append(mean_squared_error(yTrainTemp,yTrainPred, squared=False ))\n",
    "        #also need to test validation on the whole set\n",
    "        Jval.append(mean_squared_error(yval,yValPred, squared=False ))\n",
    "    #plot\n",
    "    ax.set_xlabel('Training Set Size')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    i = [int(x) for x in range(1,trainingExamples, 100)]\n",
    "    ax.plot(i,Jtrain,\"r-\",label=\"train\")\n",
    "    ax.plot(i,Jval,\"b-\",label=\"val\")\n",
    "    ax.legend(loc = 'best')\n",
    "    \n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.set_title(\"Worst Model\")\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.set_title(\"Best Model\")\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.set_title(\"Best Model 2\")\n",
    "ax2.set_ylim(0,0.05);\n",
    "ax3.set_ylim(0,0.02);\n",
    "# ax3.set_ylim(0,200);\n",
    "    \n",
    "#Learning Curvues for Random Forest \n",
    "\n",
    "#worst model, a litle underfitting\n",
    "plot_learning_curves(worstModel, X_trains, y_train, X_vals, y_val, ax1)\n",
    "#next 2 are both 100%\n",
    "plot_learning_curves(bestModel, X_trains, y_train, X_vals, y_val, ax2)\n",
    "plot_learning_curves(bestModel2, X_trains, y_train, X_vals, y_val, ax3)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d55c60",
   "metadata": {},
   "source": [
    "## Precision/Recall/F1 Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yWorst = worstModel.predict(X_tests)\n",
    "yBest = bestModel.predict(X_tests)\n",
    "yBest2 = bestModel2.predict(X_tests)\n",
    "\n",
    "\n",
    "print(f\"Worst Model: Precision = {precision_score(y_test,yWorst)},\\\n",
    "      Recall = {recall_score(y_test,yWorst)}, f1score = {f1_score(y_test,yWorst)}\")\n",
    "print(f\"Best Model: Precision = {precision_score(y_test,yBest)},\\\n",
    "      Recall = {recall_score(y_test,yBest)}, f1score = {f1_score(y_test,yBest)}\")\n",
    "print(f\"Best Model2: Precision = {precision_score(y_test,yBest2)},\\\n",
    "      Recall = {recall_score(y_test,yBest2)}, f1score = {f1_score(y_test,yBest2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606212aa",
   "metadata": {},
   "source": [
    "## Feature Importance for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e854bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilities that determine each feature's importance\n",
    "featureImportance = bestModel2.feature_importances_\n",
    "\n",
    "sortedIndices = np.argsort(featureImportance)\n",
    "descendingIndices = np.flip(sortedIndices,0)\n",
    "#probabilities in descending  order\n",
    "featureImportanceDescending = featureImportance[descendingIndices]\n",
    "#a list that goes from best -> worst features\n",
    "bestFeatures = df.columns[descendingIndices]\n",
    "    \n",
    "print(\"Here are the 10 best features\")\n",
    "print(bestFeatures[:10])\n",
    "    \n",
    "bestFeatures = bestFeatures[:10]\n",
    "featureImportanceDescending = featureImportanceDescending[:10]\n",
    "\n",
    "#BAR GRAPH OF BEST FEATURES\n",
    "plt.figure(figsize=(10,5))\n",
    "chart = sns.barplot(\n",
    "    x=bestFeatures, y=featureImportanceDescending\n",
    ")\n",
    "plt.xticks(\n",
    "    rotation=45, \n",
    "    horizontalalignment='right',\n",
    "    fontweight='light',\n",
    "    fontsize='medium'  \n",
    ")\n",
    "chart.set(title=\"Best 10 Features for Best Model 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70294c7f",
   "metadata": {},
   "source": [
    "## Data Visualization With Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bcdc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# plt.figure(figsize=(20,20))\n",
    "#Just take best 10\n",
    "bestFeatures = bestFeatures[:5]\n",
    "# print(len(bestFeatures))\n",
    "sns.pairplot(df, vars = bestFeatures)\n",
    "plt.show()\n",
    "\n",
    "#Best 2 features on scatter plot\n",
    "\n",
    "sns.scatterplot(x=df.page_rank, y=df.dns_record, hue = df.status)\n",
    "plt.show()\n",
    "\n",
    "#Best 3 features on 3d plot\n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "x = df.page_rank\n",
    "y = df.dns_record\n",
    "z = df.google_index\n",
    "\n",
    "ax.set_xlabel(\"page_rank\")\n",
    "ax.set_ylabel(\"dns_record\")\n",
    "ax.set_zlabel(\"google_index\")\n",
    "ax.scatter(x, y, z)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#histogram for phishing vs. legititmate\n",
    "sns.countplot(df['status'])\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=df.google_index, hue=df.status)\n",
    "plt.show()\n",
    "\n",
    "#Page rank\n",
    "sns.displot(df.page_rank, color = 'red', label = 'page_rank', kde = True)\n",
    "plt.legend\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Page rank vs. Phishing\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.displot(df,x=\"page_rank\",hue=\"status\",kind = \"kde\")\n",
    "plt.title('Phishing versus Page Rank')\n",
    "plt.show()\n",
    "\n",
    "#DNS Record vs Phishing\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.displot(df,x=\"dns_record\",hue=\"status\",kind = \"kde\")\n",
    "plt.title('Phishing versus DNS Record')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.displot(df,x=\"google_index\",hue=\"status\",kind = \"kde\")\n",
    "plt.title('Phishing versus Google Index')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# sns.displot(df,x=\"ratio_intRedirection\",hue=\"status\",kind = \"kde\")\n",
    "# plt.title('Phishing versus Ratio Int Redirection')\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.displot(df,x=\"domain_age\",hue=\"status\",kind = \"kde\")\n",
    "plt.title('Phishing versus Domain Age')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.displot(df,x=\"popup_window\",hue=\"status\",kind = \"kde\")\n",
    "plt.title('Phishing versus popup_window')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.displot(df,x=\"avg_word_path\",hue=\"status\",kind = \"kde\")\n",
    "plt.title('Phishing versus Avg Word Path')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.displot(df,x=\"nb_space\",hue=\"status\",kind = \"kde\")\n",
    "plt.title('Phishing versus # Space')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# sns.displot(df,x=\"url\",hue=\"status\",kind = \"kde\")\n",
    "# plt.title('Phishing versus url')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef92f1",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd791fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "# Import the necessaries libraries\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "# Set notebook mode to work in offline\n",
    "pyo.init_notebook_mode()\n",
    "indexes = [df.columns.get_loc('page_rank')-1,df.columns.get_loc('dns_record')-1,df.columns.get_loc('google_index')-1]\n",
    "print(indexes)\n",
    "\n",
    "#Make array of two best features, already scaled\n",
    "TwoBestTrain = X_train[:,[indexes[0],indexes[1]]]\n",
    "TwoBestTest = X_test[:,[indexes[0],indexes[1]]]\n",
    "\n",
    "ThreeBestTrain = X_train[:,indexes]\n",
    "ThreeBestTest = X_test[:,indexes]\n",
    "\n",
    "km1 = KMeans(n_clusters=2).fit(TwoBestTrain)\n",
    "km2  = KMeans(n_clusters=2).fit(ThreeBestTrain)\n",
    "\n",
    "\n",
    "# Check proportion of positive and negative samples\n",
    "yPred = km1.predict(TwoBestTest)\n",
    "print(\"Two Best\")\n",
    "print(f\"Number of predictions for cluster center = 0: {np.sum(yPred == 0)}\")\n",
    "print(f\"Number of predictions for cluster center = 1: {np.sum(yPred == 1)}\")\n",
    "\n",
    "#convert yPred to strings so that I get discrete colors on the graph. \n",
    "phishing = yPred.astype(str)\n",
    "#Did a reversal here. Rememeber that clustering doesn't know which is phishing/leg, it just picks cluster centers\n",
    "phishing[phishing == '1'] = 'Legitimate'\n",
    "phishing[phishing == '0'] = 'Phishing'\n",
    "\n",
    "cluster1 = km1.cluster_centers_[0]\n",
    "cluster2 = km1.cluster_centers_[1]\n",
    "\n",
    "fig = px.scatter(x=TwoBestTest[:,0], y=TwoBestTest[:,1],color = phishing,  labels={\n",
    "                     \"x\": \"Page Rank\",\n",
    "                     \"y\": \"DNS Record\"} )\n",
    "fig.add_trace(go.Scatter(mode = 'markers', x=[cluster1[0]], y=[cluster1[1]], name =\"Cluster1\",       marker=dict(\n",
    "            color = 'rgb(26, 255, 200)'\n",
    "        )\n",
    "    ))\n",
    "fig.add_trace(go.Scatter(mode = 'markers', x=[cluster2[0]], y=[cluster2[1]], name =\"Cluster2\", marker=dict(\n",
    "            color='rgb(26, 255, 26)'\n",
    "        )\n",
    "    ))\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "yPred2 = km2.predict(ThreeBestTest)\n",
    "print(\"Three Best\")\n",
    "print(f\"Number of predictions for cluster center = 0: {np.sum(yPred2 == 0)}\")\n",
    "print(f\"Number of predictions for cluster center = 1: {np.sum(yPred2 == 1)}\")\n",
    "\n",
    "#convert yPred to strings so that I get discrete colors on the graph. \n",
    "phishing2 = yPred2.astype(str)\n",
    "#Did a reversal here. Rememeber that clustering doesn't know which is phishing/leg, it just picks cluster centers\n",
    "phishing2[phishing2 == '0'] = 'Legitimate'\n",
    "phishing2[phishing2 == '1'] = 'Phishing'\n",
    "# print(phishing)\n",
    "## Plot ThreeBest and cluster centers\n",
    "fig = px.scatter_3d(x=ThreeBestTest[:,0], y=ThreeBestTest[:,1], z=ThreeBestTest[:,2],\n",
    "                    color = phishing, color_continuous_scale=px.colors.sequential.Bluered,\n",
    "                    # title=\"Clustering: Best 3 Features\",\n",
    "labels={\n",
    "                     \"x\": \"Page Rank\",\n",
    "                     \"y\": \"DNS Record\",\n",
    "                     \"z\": \"Google Index\"\n",
    "                 })\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "fig.update_xaxes(tickfont=dict(family='Rockwell', size=2))\n",
    "\n",
    "cluster1 = km2.cluster_centers_[0]\n",
    "cluster2 = km2.cluster_centers_[1]\n",
    "fig.add_trace(go.Scatter3d(mode = 'markers', x=[cluster1[0]], y=[cluster1[1]], z=[cluster1[1]], name =\"Cluster1\",       marker=dict(\n",
    "            color = 'rgb(26, 255, 200)'\n",
    "        )\n",
    "    ))\n",
    "fig.add_trace(go.Scatter3d(mode = 'markers', x=[cluster2[0]], y=[cluster2[1]], z=[cluster2[1]], name =\"Cluster2\", marker=dict(\n",
    "            color='rgb(26, 255, 26)'\n",
    "        )\n",
    "    ))\n",
    "\n",
    "fig.show()\n",
    "# print(km2.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad24ab1",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9662b94f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svmS = StandardScaler()\n",
    "X_trains = svmS.fit_transform(X_train)\n",
    "X_vals = svmS.transform(X_val)\n",
    "X_tests  = svmS.transform(X_test)\n",
    "\n",
    "def takeScore(model):\n",
    "    return model.score(X_vals, y_val.flatten())\n",
    "\n",
    "#literally no change in val when C > 2, 2 is the max\n",
    "cVals = [0.01, 0.03, 0.1, 0.3, 1, 2]\n",
    "#Try with higher gamma\n",
    "gammaVals = [1, 5, 20, 50, 100, 200]\n",
    "score = 0\n",
    "wScore = 1\n",
    "\n",
    "models = []\n",
    "\n",
    "bestParams = []\n",
    "worstParams = []\n",
    "#Linear kernel\n",
    "print(\"Values for linear kernel\")\n",
    "for i in gammaVals:\n",
    "    gamma = 1/(i**2)\n",
    "    for j in cVals:\n",
    "        model = svm.SVC(C=j, gamma=gamma)\n",
    "        model.fit(X_trains, y_train.flatten())\n",
    "        if model.score(X_vals, y_val.flatten()) > score:\n",
    "            score = model.score(X_vals, y_val.flatten())\n",
    "            bestParams = [\"linear\", j, i]\n",
    "        if model.score(X_vals, y_val.flatten()) < wScore:\n",
    "            wScore = model.score(X_vals, y_val.flatten())\n",
    "            worstParams = [\"linear\", j, i]\n",
    "        models.append(model)\n",
    "        print(f\"Score for linear kernel with C = {j} and sigma = {gamma}: Train data - {model.score(X_trains, y_train.flatten())} Val data - {model.score(X_vals, y_val.flatten())}\")\n",
    "        \n",
    "#rbf kernel\n",
    "print(\"\\nValues for rbf kernel\")\n",
    "for i in gammaVals:\n",
    "    gamma = 1/(i**2)\n",
    "    for j in cVals:\n",
    "        model = svm.SVC(kernel='rbf', C=j, gamma=gamma)\n",
    "        model.fit(X_trains, y_train.flatten())\n",
    "        if model.score(X_vals, y_val.flatten()) > score:\n",
    "            score = model.score(X_vals, y_val.flatten())\n",
    "            bestParams = [\"rbf\", j, i]\n",
    "        if model.score(X_vals, y_val.flatten()) < wScore:\n",
    "            wScore = model.score(X_vals, y_val.flatten())\n",
    "            worstParams = [\"rbf\", j, i]\n",
    "        models.append(model)\n",
    "        print(f\"Score for rbf kernel with C = {j} and sigma = {gamma}: Train data - {model.score(X_trains, y_train.flatten())} Val data - {model.score(X_vals, y_val.flatten())}\")\n",
    "        \n",
    "print(f\"\\nBest params: kernel = {bestParams[0]}, C = {bestParams[1]}, gamma = {bestParams[2]}\")\n",
    "print(f\"\\nWorst params: kernel = {worstParams[0]}, C = {worstParams[1]}, gamma = {worstParams[2]}\")\n",
    "bestModelSVM = svm.SVC(kernel=bestParams[0], C=bestParams[1], gamma=bestParams[2])\n",
    "worstModelSVM = svm.SVC(kernel=worstParams[0], C=worstParams[1], gamma=worstParams[2])\n",
    "\n",
    "models.sort(key=takeScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1845f3-3b6e-41e0-a3cf-89c4ce5add08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Precision, Recall, F1Score\n",
    "bestModelSVM.fit(X_trains, y_train.flatten())\n",
    "yBest = bestModelSVM.predict(X_tests)\n",
    "print(f\"Best Model: Precision = {precision_score(y_test,yBest)},\\\n",
    "      Recall = {recall_score(y_test,yBest)}, f1score = {f1_score(y_test,yBest)}\")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.set_title(\"Best Model\")\n",
    "\n",
    "\n",
    "plot_learning_curves(bestModelSVM, X_trains, y_train, X_vals, y_val, ax1)\n",
    "\n",
    "worstModelSVM.fit(X_trains, y_train.flatten())\n",
    "yWorst = worstModelSVM.predict(X_tests)\n",
    "print(f\"Worst Model: Precision = {precision_score(y_test,yWorst)},\\\n",
    "      Recall = {recall_score(y_test,yWorst)}, f1score = {f1_score(y_test,yWorst)}\")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.set_title(\"Worst Model\")\n",
    "\n",
    "\n",
    "plot_learning_curves(worstModelSVM, X_trains, y_train, X_vals, y_val, ax1)\n",
    "\n",
    "for i in models:\n",
    "    yTemp = i.predict(X_tests)\n",
    "    print(f\"Precision = {precision_score(y_test,yTemp)},\\\n",
    "      Recall = {recall_score(y_test, yTemp)}, f1score = {f1_score(y_test,yTemp)}\")\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(14, 5))\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    ax1 = fig.add_subplot(131)\n",
    "\n",
    "\n",
    "    plot_learning_curves(i, X_trains, y_train, X_vals, y_val, ax1)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b609dbff",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logRS = StandardScaler()\n",
    "X_trains = logRS.fit_transform(X_train)\n",
    "X_vals = logRS.transform(X_val)\n",
    "X_tests  = logRS.transform(X_test)\n",
    "\n",
    "cVals = [0.01, 0.03, 0.1, 0.3, 1, 2, 5, 10]\n",
    "\n",
    "score = -1\n",
    "bestParams = []\n",
    "#Logistic regression with l2\n",
    "print(\"Logistic regression using l2 penalty:\")\n",
    "for i in cVals:\n",
    "    model = linear_model.LogisticRegression(C=i, solver='saga', max_iter=1000)\n",
    "    model.fit(X_trains, y_train)\n",
    "    print(f\"Logistic regression using l2 and C={i}: Train - {model.score(X_trains, y_train)}; Val - {model.score(X_vals, y_val)}\")\n",
    "    if model.score(X_vals, y_val) > score or score == -1:\n",
    "        score = model.score(X_vals, y_val)\n",
    "        bestParams = ['l2', i]\n",
    "    \n",
    "\n",
    "#Logistic regression with l1\n",
    "print(\"\\nLogistic regression using l1 penalty:\")\n",
    "for i in cVals:\n",
    "    model = linear_model.LogisticRegression(C=i, penalty='l1', solver='saga', max_iter=1000)\n",
    "    model.fit(X_trains, y_train)\n",
    "    print(f\"Logistic regression using l1 and C={i}: Train - {model.score(X_trains, y_train)}; Val - {model.score(X_vals, y_val)}\")\n",
    "    if model.score(X_vals, y_val) > score or score == -1:\n",
    "        score = model.score(X_vals, y_val)\n",
    "        bestParams = ['l1', i]\n",
    "        \n",
    "#Logistic regression with elastic net:\n",
    "print(\"\\nLogistic regression using elastic net:\")\n",
    "for i in cVals:\n",
    "    model = linear_model.LogisticRegression(C=i, penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
    "    model.fit(X_trains, y_train)\n",
    "    print(f\"Logistic regression using elastic net and C={i}: Train - {model.score(X_trains, y_train)}; Val - {model.score(X_vals, y_val)}\")\n",
    "    if model.score(X_vals, y_val) > score or score == -1:\n",
    "        score = model.score(X_vals, y_val)\n",
    "        bestParams = ['elasticnet', i]\n",
    "        \n",
    "print(f\"\\nThe best performing model uses the {bestParams[0]} penalty with C={bestParams[1]}\")\n",
    "\n",
    "bestModelLR = linear_model.LogisticRegression(C=bestParams[1], penalty=bestParams[0], solver='saga', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f0528-ad10-472f-899a-1504be0a1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Precision, Recall, F1Score\n",
    "bestModelLR.fit(X_trains, y_train)\n",
    "yBest = bestModelLR.predict(X_tests)\n",
    "print(f\"Best Model: Precision = {precision_score(y_test,yBest)},\\\n",
    "      Recall = {recall_score(y_test,yBest)}, f1score = {f1_score(y_test,yBest)}\")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.set_title(\"Best Model\")\n",
    "\n",
    "plot_learning_curves(bestModelLR, X_trains, y_train, X_vals, y_val, ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df73279-0e0b-4dae-84b0-31439cc184d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_best = X_train[:,:2]\n",
    "X_best_pos = X_best[y_train == 1]\n",
    "X_best_neg = X_best[y_train == 0]\n",
    "\n",
    "\n",
    "#plt.scatter(X_best_pos[:,0:1], X_best_pos[:,1:2], color='b')\n",
    "plt.scatter(X_best_neg[:,0:1], X_best_neg[:,1:2], color='r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
